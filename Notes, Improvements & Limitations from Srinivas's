<<<< Notes, Improvements & Limitations from Srinivas's Thesis for Incremental Bouquet >>>>

NOTES

Outdated statistics, coarse summaries, attribute value independence assumptions, complex user-defined predicates, and error propagation in the query plan tree
In ETL workload, statistics may actually be unavailable
Huge compile time effort for guarantee computation, Impact of Dimensionality, Platform independence
PB does Hypograph pruning, SB does Half-Space pruning via Spilling
Downstream nodes are not executed, Spill node indentification should be clear
Independence of Cost model, Simulation will work for my experiments
Demand driver Iterator model, no concurrency inter- or intra-pipeline
Spillbound doesn't used Anorexic reduction anywhere, however DimRed uses relaxation in similar way
1D is always Plan bouquet, as spilling will have no use for 1D
ESS is Subset of PSS, trivial predicates will also arise in case of scalable plan boquet


IMPROVEMENTS & LIMITATIONS
5D_Q29_DS is 5 Dim, Query 29 TPC-DS
Origin & Terminus, how to specify, in either of AP/GP
Total order conversion for Spill-node indentification is issue for implementation
Removal of Repeat executions and Multiple merged execution post spill node
Concept of Repeat execution, as it contributes significantly to bound provided by SpillBound
Can we remove partially, "Selectivity Independence" at Contour level for selectivity discovery??
Should we Spill at 1st node itself?? Can anything from Plan Bouquet (much partial plan execution) be still used??
NEXUS too doesn't have safety check that if cost lies withing [CC,CC(1+alpha)], what if not

PCM Assumption Issues, if Violated, how and approximation fit is used, without knowing whole of OCS
In practice, however, we observe minor violations in monotonicity and smoothness assumptions.
We overcome this issue by fitting a continuous monotonic function to the actual function.
Empirically we see that this fitting can achieved with small errors

Validation of APC
Further, even the rare violations that surfaced were found to be artifacts of rounding
errors, cost-modeling errors, and occasional PCM violations due to the PostgreSQL query optimizer
not being entirely cost-based in character.


Aligned bound is achieved in restricted set of environments, I am leaving that for now
FrugalSB is meant for Ad-hoc queries, exponential reduction, for linear increase in MSO
What if MSO relaxation is larger than Cost doubling ratio, issues for sure in 1D FrugalSB
Taking a finite resolution say R=100, is it ok to take 1% on Terabyte database will itself be too large an assumption
Because 0 is never considered


1(a). Regarding the APL (Axis Parallel Linear) behavior of OCS.
Do we need to prove that OCS follows APL in theory?

1(b). Can you explain point below (from your Thesis, Sanket's work)
*Piecewise APL Fit of OCS*
It is important point to note here is that we do not require an accurate quantitative fit, but only an
accurate qualitative fit – that is, the slope behavior should be adequately captured

2(a). For generation of Iso-cost contours for SB and DimRed techniques, out of "NEXUS" OR "Full ESS exploration" which one is used during last experiments of SpillBound? Also, what are resolution values used during experiments for each n-Dimensional query (e.g., 10,000 for 1D queires, 300 fir 2D & so on)?

2(b). While criticizing loss of expected improvement from NEXUS over full ESS exploration in chapter 7 of your thesis. You've mentioned that
	(i)  If large number of contours need to be drawn, then NEXUS effectively is close to full space exploration
	(ii) Randomized contour placement as suggested by Anshuman for fairness needs more contours to be drawn again increasing number of contours need to be drawn by NEXUS. 
	(iii) If through domain knowledge, lower bound of selecitvity is known on any predicate, Spillbound will be able to use that knowledge, while all contours of NEXUS needs to be redrawn from scratch.

	At first, I don't think this will be the case, re-drawing contours will not be necessary while using either NEXUS or full-space enumeration.
	Also, I agree in theory NEXUS has complexity of O(M*D*(RES^D)) while Full space enumeration needs O(RES^Dim).
	In general NEXUS has speed up of O(RES/(M*D)), but during proposal of NEXUS, a common assumption is that RES is sufficiently high such that we should be able to find a point in interval [C,(1+a)*C], for sufficiently small a like 0.05.

	While for making things computationally feasible for higher dimensions like five, experiments are run at lower resolutions around 30, does it still guarantee in some way, that we will get points in interval [C, (1+a)*C]. If now how that impact of using low resolution bounded? (I have got this while reading Sriram's report, where cost deviation of contours discovered from NEXUS is also beyond 5%)


3. During schematic removal, I am not clear on how lower bound in case of conjuction predicates can be calculated? (Even an example will work for me, a formula will be much better)

4. During schematic removal, How upper & lower bounds on selectivity are used for predicate removal? I am not clear with a statement that is mentioned in Your Thesis (mentioned below)
*Quote : 6.4 Schematic Removal of Dimensions*
After determining the bounds of the relational selectivity, we vary the individual predicates between [0, 1], discarding any selectivity combinations that violate the relational selectivity bounds.

5. It can be the case that dimensions removed by MaxSelRemoval need to be brought into picture in case of dynamic sized databases, which will make ESS a non-regular hypercube. Say, initial axis has selectivity [ℇ,1] with geometric distribution of selectivity values on each axis, which will upon scale up become [ℇ, 1+Δ]

6. Finding partitions of OCS using K-Subspace clustering (which has O(n**2) complexity) is followed by Fitting Piecewise APL to each of partition obtained. Is this fitting of piecewise linear function is what you are talking about, for handling PCM & smoothness violation in starting of the thesis??
*Quote:  3.4 Assumptions (PCM Assumption)*
In practice, however, we observe minor violations in monotonicity and smoothness assumptions.
We overcome this issue by fitting a continuous monotonic function to the actual function.
Empirically we see that this fitting can achieved with smallion errors

7. Is a multi-core version of ESS constrcution written for empirical calculations?


*Questions & Points to ponder into Report*
1. Choice of picture of viewing changed database? [0,1] or [0,1+1+Δ] is better for framing problem for incremental algorithm design
2. Any incremental robust 2D algorithm, will be generalizable to n-Dimensional version too
3. Cons of using Low resolution (which is done in case of High dimensional ESS, will affect both NEXUS & Full space)
4. Need & Design of Exponential NEXUS for generating contours
5. Is high resolution with Exp NEXUS the only solution, can we mix Uniform & Geometric progression using slope behavior for obtaining guarantee on re-compilation complexity and bounds on MSO guarantee in Incremental bouquet algorithms


*Points later to be Explored from work till now in Robust Query Processing from DSL*
1. Fitting such a curve fit for handling violations of PCM & Smoothness assumption, will need full space exploration itself, and doing so for incremental bouquet based technique will create same complexity as re-compilation
2. Which of Dimensionality reduction techniques are applicable to Dynamic databases
3. Is FSB somewhat utilizing what Exponential search based technique will do in NEXUS
4. WeakDimRed is not compilation, but execution improvement and can always be added to any incremental bouquet algoithm


Lower bound of Conjunction predicates?





#### DimRed ####
SchematicRed > MaxSelRed > WeakDimRed (Empirical Only)
Overhead reduction, MSO minimization are two different things achieved in 3 stage pipeline

# SchematicRed
If not accurate, tighter lower & upper bounds
The above discussion was for individual predicates. However, in general, there may be multiple filter predicates on a base relation. In such cases, we first compute the ranges or values for each individual predicate (in the manner discussed above), and then use these individual bounds to compute bounds on the relational selectivity as a whole.

How to find tighter upper & lower bounds in case of both Conjuctive & Disjunctive predicates?? (string predicates, I am leaving as of now)
How bounds are used for selectivity predicate removal??

# MaxSel
Don't know max aprior on either or predicate in case of scalable databases, dimension may need to be expanded later
Still, APL assumption says that endpoint has extreme cost,
Fitting Piecewise APL using K-Subspace clustering (derived from K-Means) has O(n**2) complexity, can we improve upon this using Neural network based approaches?
Is this fitting of piecewise linear function is what Srinivas is talking about, for PCM violation??

# WeakDimRed
Instead of choosing each subplan for each predicate, some plans swallows others, some predicates are removed from search for current countour, results in a seconds inflation factor due to Plan swallowing, not Anorexic Reduction like, but MSO based removal of Dim
Plan with least Inflation factor is choosen by FPC

n-Dimensional version still not clear (ye kabhi hoga kisi algo ka ya nhi, chal kya rha h jeevan m)